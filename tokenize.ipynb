{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from animeGPT.tokenizer import Tokenizer\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath(os.path.join(os.getcwd()))\n",
    "details_path = os.path.join(dir_path, 'data/details.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses = []\n",
    "titles = []\n",
    "genres = []\n",
    "with open(details_path, 'r') as file:\n",
    "    anime_details = json.load(file)\n",
    "    for anime in anime_details['anime']:\n",
    "        synopses.append(anime['synopsis'])\n",
    "        titles.append(anime['title'])\n",
    "        try :\n",
    "            genres.append([genre['name'] for genre in anime['genres']])\n",
    "        except KeyError:\n",
    "            genres.append([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_synopsis(synopsis):\n",
    "    # Remove line breaks and extra whitespace\n",
    "    synopsis = re.sub(r'\\s+', ' ', synopsis)\n",
    "    # Remove leading and trailing whitespace\n",
    "    synopsis = synopsis.strip()\n",
    "    return synopsis\n",
    "\n",
    "def prepare_text_file(samples, filename):\n",
    "    # Concatenate all the samples into a single string\n",
    "    text = '\\n'.join(samples)\n",
    "    \n",
    "    # Write the concatenated string to a text file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "        \n",
    "\n",
    "def contains_non_english(text):\n",
    "    # Define a regular expression pattern to match non-English characters\n",
    "    non_english_pattern = re.compile(r'[^\\x00-\\x7F]')\n",
    "    \n",
    "    # Check if the text contains any non-English characters\n",
    "    if non_english_pattern.search(text):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses = [preprocess_synopsis(synopsis) for synopsis in synopses]\n",
    "non_english_indexes = []\n",
    "for i, sample in enumerate(synopses):\n",
    "    if contains_non_english(sample) == True:\n",
    "        non_english_indexes.append(i)\n",
    "non_english_indexes = sorted(non_english_indexes, reverse=True)\n",
    "for idx in non_english_indexes:\n",
    "    del synopses[idx]\n",
    "    del titles[idx]\n",
    "    del genres[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [', '.join(sub) for sub in genres]\n",
    "    \n",
    "train_text = (titles + genres + synopses)\n",
    "train_text = ', '.join(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "animeTokenizer = Tokenizer()\n",
    "animeTokenizer.train(train_text, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    '<t>' : 5000,\n",
    "    '</t>' : 5001,\n",
    "    '<g>' : 5002,\n",
    "    '</g>' : 5003,\n",
    "    '<s>' : 5004,\n",
    "    '</s>' : 5005\n",
    "}\n",
    "animeTokenizer.register_special_tokens(special_tokens)\n",
    "animeTokenizer.save('animeTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "animeTokenizer.load('animeTokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(animeTokenizer.encode(synopses[0], 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(s) for s in synopses]\n",
    "np.percentile(lengths, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, synopsis in enumerate(synopses):\n",
    "    if len(synopsis) <= np.percentile(lengths, 10):\n",
    "        del synopses[idx]\n",
    "        del titles[idx]\n",
    "        del genres[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses = [f'<s>{sample}</s>' for sample in synopses]\n",
    "titles = [f'<t>{title}</t>' for title in titles]\n",
    "genres = [f'<g>{genre}</g>' for genre in genres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Centuries ago, mankind was slaughtered to near extinction by monstrous humanoid creatures called Titans, forcing humans to hide in fear behind enormous concentric walls. What makes these giants truly terrifying is that their taste for human flesh is not born out of hunger but what appears to be out of pleasure. To ensure their survival, the remnants of humanity began living within defensive barriers, resulting in one hundred years without a single titan encounter. However, that fragile calm is soon shattered when a colossal Titan manages to breach the supposedly impregnable outer wall, reigniting the fight for survival against the man-eating abominations. After witnessing a horrific personal loss at the hands of the invading creatures, Eren Yeager dedicates his life to their eradication by enlisting into the Survey Corps, an elite military unit that combats the merciless humanoids outside the protection of the walls. Eren, his adopted sister Mikasa Ackerman, and his childhood friend Armin Arlert join the brutal war against the Titans and race to discover a way of defeating them before the last walls are breached. [Written by MAL Rewrite]</s>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synopses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "for s, t, g in zip(synopses, titles, genres):\n",
    "    input_data.extend([t, g, s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<t>Ansatsu Kyoushitsu 2nd Season</t>\n",
      "<g>Action, Comedy, School, Shounen</g>\n",
      "<s>Returning from their summer vacation, the students of Class 3-E at the prestigious Kunugigaoka Middle School find themselves up against unbeatable odds. Faced with the possibility of world annihilation, the students must come up with increasingly elaborate and creative ways to kill their teacher, the cunning yet optimistic and helpful Koro-sensei. However, eliminating Koro-sensei is not the only objective the students need to worry about. Gakuhou Asano, the academy's merciless and cruel principal, seeks to prevent Class 3-E's success by brainwashing his other hard-working pupils into ruthlessly competitive studying machines. Hostility begins to linger in the air as traitors and killers alike attempt to claim the bounty on Koro-sensei's head for themselves. Nagisa Shiota, one of Class 3-E's most skilled assassins, finds himself in the middle of the conflict. While he works to maintain his academic standing and prevent the end of the world, domestic affairs jeopardize his place in Class 3-E. Together with his dedicated classmates, he must now face the threats head-on. [Written by MAL Rewrite]</s>\n"
     ]
    }
   ],
   "source": [
    "print(input_data[99])\n",
    "print(input_data[100])\n",
    "print(input_data[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000, 65, 110, 115, 989, 1225, 409, 104, 1543, 32, 50, 1519, 1390, 2001]\n",
      "[2000, 65, 110, 115, 989, 1225, 409, 104, 1543, 32, 50, 1519, 1390, 2001]\n"
     ]
    }
   ],
   "source": [
    "print(animeTokenizer.encode(titles[33], 'all'))\n",
    "print(animeTokenizer.encode(input_data[99], 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_text_file(input_data, 'input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
